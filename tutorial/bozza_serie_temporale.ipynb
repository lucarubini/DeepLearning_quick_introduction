{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bozza_serie_temporale.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfvTKGZPzQ+HOCub0fV1+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarubini/DeepLearning_quick_introduction/blob/main/tutorial/bozza_serie_temporale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importare le librerie necessarie\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dfQcciMjIEsD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Crea Dati**"
      ],
      "metadata": {
        "id": "EpmfLddYIFWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configurazioni per i dati sintetici\n",
        "\n",
        "N_lines = 1000\n",
        "N_products = 13\n",
        "max_quantity = 90 #massima quantita di prodotti\n",
        "\n",
        "filename_output = 'data.csv'"
      ],
      "metadata": {
        "id": "S7BXfXS_I978"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open the file in the write mode\n",
        "with open(filename_output, 'w', encoding='UTF8') as f:\n",
        "    # create the csv writer\n",
        "    writer = csv.writer(f)\n",
        "    header = [f\"prod_{i}\" for i in range(N_products)]\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # write a row to the csv file\n",
        "    for _ in range(N_lines):\n",
        "      row = [np.random.randint(max_quantity) for _ in range(N_products)]\n",
        "      writer.writerow(row)"
      ],
      "metadata": {
        "id": "pcxudDPJIEub"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_PoC9mm-IEzV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importare i Dati**"
      ],
      "metadata": {
        "id": "2t3GHpy5MHAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configurations\n",
        "csv_path = 'data.csv'"
      ],
      "metadata": {
        "id": "hkVazeBcMbB3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "-rUCi6xBMGVy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.describe().transpose()"
      ],
      "metadata": {
        "id": "GwPj455WrDDt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "num_features = df.shape[1]\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]"
      ],
      "metadata": {
        "id": "FSDqyb3MzBGa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "val_df = (val_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std"
      ],
      "metadata": {
        "id": "L6vUQZCKMGdq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df.describe().transpose()"
      ],
      "metadata": {
        "id": "e2mnz5fezG7P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VffOnjTKzWhS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classe per generare le finestre di dati\n",
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])\n",
        "  \n",
        "  def split_window(self, features):\n",
        "    inputs = features[:, self.input_slice, :]\n",
        "    labels = features[:, self.labels_slice, :]\n",
        "    if self.label_columns is not None:\n",
        "      labels = tf.stack(\n",
        "          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "          axis=-1)\n",
        "\n",
        "    # Slicing doesn't preserve static shape information, so set the shapes\n",
        "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "    inputs.set_shape([None, self.input_width, None])\n",
        "    labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "  def make_dataset(self, data):\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=None,\n",
        "        sequence_length=self.total_window_size,\n",
        "        sequence_stride=1,\n",
        "        shuffle=True,\n",
        "        batch_size=32,)\n",
        "    ds = ds.map(self.split_window)\n",
        "    return ds\n",
        "\n",
        "  @property\n",
        "  def train(self):\n",
        "    return self.make_dataset(self.train_df)\n",
        "\n",
        "  @property\n",
        "  def val(self):\n",
        "    return self.make_dataset(self.val_df)\n",
        "\n",
        "  @property\n",
        "  def test(self):\n",
        "    return self.make_dataset(self.test_df)\n",
        "\n",
        "  @property\n",
        "  def example(self):\n",
        "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "    result = getattr(self, '_example', None)\n",
        "    if result is None:\n",
        "      # No example batch was found, so get one from the `.train` dataset\n",
        "      result = next(iter(self.train))\n",
        "      # And cache it for next time\n",
        "      self._example = result\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "t3njqWiwzhuQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2 = WindowGenerator(input_width=10, label_width=4, shift=4,\n",
        "                     label_columns=[f\"prod_{i}\" for i in range(13)])\n",
        "w2"
      ],
      "metadata": {
        "id": "wRQRaw7Sr1fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack three slices, the length of the total window.\n",
        "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
        "                           np.array(train_df[100:100+w2.total_window_size]),\n",
        "                           np.array(train_df[200:200+w2.total_window_size])])\n",
        "\n",
        "example_inputs, example_labels = w2.split_window(example_window)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print(f'Window shape: {example_window.shape}')\n",
        "print(f'Inputs shape: {example_inputs.shape}')\n",
        "print(f'Labels shape: {example_labels.shape}')"
      ],
      "metadata": {
        "id": "aZndn0oPrrVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2.train.element_spec"
      ],
      "metadata": {
        "id": "BZug6dZ5XgE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_inputs, example_labels in w2.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ],
      "metadata": {
        "id": "_o0oVC7Xs_zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_labels"
      ],
      "metadata": {
        "id": "4TU09PCn4zor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Crea e Addestra un modello**"
      ],
      "metadata": {
        "id": "fRcprJjd2ACU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCHS = 20\n",
        "\n",
        "def compile_and_fit(model, window, patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                optimizer=tf.optimizers.Adam(),\n",
        "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=[early_stopping])\n",
        "  return history"
      ],
      "metadata": {
        "id": "5w9qB9rPXgHb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_STEPS=4\n",
        "INPUT_STEP=6\n",
        "multi_window = WindowGenerator(input_width=INPUT_STEP,\n",
        "                               label_width=OUT_STEPS,\n",
        "                               shift=OUT_STEPS,\n",
        "                               label_columns=[f\"prod_{i}\" for i in range(13)])\n",
        "print(multi_window)\n",
        "\n"
      ],
      "metadata": {
        "id": "LwtTMSkRXgPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_lstm_model = tf.keras.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, lstm_units].\n",
        "    # Adding more `lstm_units` just overfits more quickly.\n",
        "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "    # Shape => [batch, out_steps*features].\n",
        "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
        "                          kernel_initializer=tf.initializers.zeros()),\n",
        "    # Shape => [batch, out_steps, features].\n",
        "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
        "])"
      ],
      "metadata": {
        "id": "iydVY6g6XgRr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = compile_and_fit(multi_lstm_model, multi_window)"
      ],
      "metadata": {
        "id": "iqpMkB2tXgXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_val = multi_lstm_model.evaluate(multi_window.val)"
      ],
      "metadata": {
        "id": "SqyFtMuvXgab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_test = multi_lstm_model.evaluate(multi_window.test)"
      ],
      "metadata": {
        "id": "g0ZvW7Qu4P5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calcolo singolo\n",
        "A_example = tf.convert_to_tensor(np.random.rand(1,INPUT_STEP,num_features))"
      ],
      "metadata": {
        "id": "N714rWF2z-s3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_example"
      ],
      "metadata": {
        "id": "HRf9vM6Lz-x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_A = multi_lstm_model(A_example)"
      ],
      "metadata": {
        "id": "R5fRntwgz-0f"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_A.shape"
      ],
      "metadata": {
        "id": "gi2Yvdm9z-2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_A[0,0,:]"
      ],
      "metadata": {
        "id": "ilhrqevh1nkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kU0PgdHZ1wgo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}