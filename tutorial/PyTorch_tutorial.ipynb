{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhArtp0pPkaBEwdcTLCqL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucarubini/DeepLearning_quick_introduction/blob/main/tutorial/PyTorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRJEFxAYBSQZ"
      },
      "source": [
        "# Set up PyTorch\n",
        "\n",
        "(Questo tutorial e' basato su quello proposto nella documentazione/guida della libreria [Pytorch tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) )\n",
        "\n",
        "Importiamo `Pytorch` e tutte le librerie necessarie nel  nostro notebook per iniziare il tutorial:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6bOD_emBK23"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4pD6ShEW4Y"
      },
      "source": [
        "# Load Dataset\n",
        "\n",
        "Per questo tutorial useremo il dataset gia' configurato **MNIST**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpS_0pYABb3X"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txZLnNHEKmC0"
      },
      "source": [
        "Istanziamo le classi per fornire al modello i dati durante il training e il test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7xMY6euEZay"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWiM7gtiJVAh"
      },
      "source": [
        "Osserviamo ora come sono fatti i dati che utilizzeremo, osserviamo come la classe fornisce tensori la cui prima dimensione coincide con il `batch_size`, cioe' il numero di dati da considerare contemporaneamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "3-vUPuy0JbAN",
        "outputId": "e0c8e9a6-84fb-44c1-eb02-e8204780e49f"
      },
      "source": [
        "#Show Image\n",
        "image_test_idx = 42\n",
        "plt.imshow(test_data[image_test_idx][0][0])\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    print(\"Target: {}\".format(y[image_test_idx]))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n",
            "Target: 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcodb2ANKNa1"
      },
      "source": [
        "Contiamo ora il numero di esempi per ciascuna partizione del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN5J71PnFAJj",
        "outputId": "0b6ce56c-6ca2-48a4-ca8d-1bc2202b8e41"
      },
      "source": [
        "#Count Elements\n",
        "print(\"Number of train samples: {}\".format(training_data.targets.numel()))\n",
        "print(\"Number of test samples: {}\".format(test_data.targets.numel()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples: 60000\n",
            "Number of test samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZmkruwsKTU2"
      },
      "source": [
        "`PyTorch` necessita di specificare se si sta lavorando su gpu o cpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_p6O4LZEyqq",
        "outputId": "edd70d4f-3a79-4ce9-fe54-31899d5c25d1"
      },
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrudh0a7FGjZ"
      },
      "source": [
        "# Build a (simple) machine learning mode\n",
        "Costruiamo ora il modello definendo una classe apposita in cui implementiamo il metodo `__init__` per inizializzare il modello e poi il metodo `forward` per valutare il modello. \n",
        "\n",
        "Per creare il modello useremo il modulo `nn.Sequential`, in cui diversi layer saranno impilati.\n",
        "\n",
        "L'idea per l'architettura del modello e' quella di trasformare ogni immagine in un array, 'appiatendolo' e poi configurare una rete neurale\n",
        "\n",
        "  ![One prediction 24 hours into the future.](https://andreaprovino.it/wp-content/uploads/2020/09/tensorflow-neural-network-schema-tensorflow-mnist-tutorial-italiano-esempio-guida-tensorflow-italia-tensorflow-classification-hello-world-single-digit.png)\n",
        "\n",
        "Tipi di layers utilizzati:\n",
        "- nn.Flatten: https://pytorch.org/docs/master/generated/torch.nn.Flatten.html#flatten\n",
        "- nn.Sequential: https://pytorch.org/docs/master/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential\n",
        "- nn.Linear: https://pytorch.org/docs/master/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear\n",
        "- nn.ReLU: https://pytorch.org/docs/master/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74YNTTaE0x7"
      },
      "source": [
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: input istance to be flattened\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROLdAyeKLznT"
      },
      "source": [
        "Collochiamo il modello sul corretto device e vediamo come il modello e' costituito"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQUlbMPtL7r_",
        "outputId": "b6cbecfe-33d9-4acc-851e-36f564283786"
      },
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyvYUwDCFYxE"
      },
      "source": [
        "Verifichiamo ra la dimensionalita' del modello, ispezionando come i paragrafi sono implelentati al loro interno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo_x9ZJ_FUYB",
        "outputId": "782fbe9d-b30d-44f2-d664-285f07f47203"
      },
      "source": [
        "#Count model size\n",
        "#from parameters\n",
        "count_model_params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "for W in model.parameters():\n",
        "    print(W)\n",
        "\n",
        "for i,W in enumerate(model.parameters()):\n",
        "    print(\"W_{}.shape: {}\".format(i,W.shape))\n",
        "#manually (+1 is due to bias)\n",
        "count_check_params=(784+1)*128+(128+1)*10\n",
        "\n",
        "print(\"Model Parameters check: (784+1)*128+(128+1)*10\")\n",
        "print(\"Model Parameters count: {}\".format(count_model_params))\n",
        "print(\"Model Check count: {}\".format(count_check_params))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0263,  0.0185,  0.0023,  ..., -0.0062, -0.0217, -0.0125],\n",
            "        [ 0.0161,  0.0189, -0.0325,  ..., -0.0031,  0.0071, -0.0354],\n",
            "        [ 0.0186, -0.0014, -0.0114,  ..., -0.0021, -0.0095, -0.0344],\n",
            "        ...,\n",
            "        [ 0.0024, -0.0103,  0.0147,  ..., -0.0077, -0.0115,  0.0262],\n",
            "        [-0.0329,  0.0186, -0.0140,  ..., -0.0335, -0.0088, -0.0044],\n",
            "        [ 0.0025, -0.0297,  0.0119,  ..., -0.0006,  0.0344, -0.0197]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-1.1690e-02, -2.0543e-02,  2.5726e-02, -2.2077e-02, -1.8072e-02,\n",
            "        -1.0465e-02, -1.9939e-02,  2.2652e-02, -3.4591e-02, -1.6724e-02,\n",
            "        -3.5402e-02,  2.4830e-02, -1.0513e-02, -1.4199e-02,  3.3653e-02,\n",
            "        -2.1050e-02, -2.2737e-02,  3.2275e-02,  8.3372e-03,  2.7036e-02,\n",
            "         1.2205e-02,  1.4913e-02,  4.6065e-03, -2.8627e-02, -2.0004e-02,\n",
            "        -1.7942e-02, -1.4370e-02,  1.7746e-02, -2.9038e-02,  2.9946e-02,\n",
            "        -8.8730e-05, -1.4155e-02,  4.2788e-06,  1.1043e-02, -3.3394e-02,\n",
            "        -3.2510e-02, -1.4782e-02, -1.4390e-02, -3.4989e-02,  1.9720e-02,\n",
            "        -3.8979e-03,  1.3986e-02,  1.0555e-02,  2.0049e-02,  3.3023e-02,\n",
            "        -1.2145e-02,  2.5003e-02, -1.7986e-02, -3.1396e-02,  2.5854e-02,\n",
            "        -1.4956e-02, -1.2575e-02,  2.8478e-02, -6.1785e-04, -3.3846e-02,\n",
            "        -3.3130e-02,  2.6696e-02,  5.2226e-03,  2.7536e-02, -1.2530e-02,\n",
            "         1.7874e-02,  2.8874e-02,  2.0976e-02,  7.2884e-04, -9.6759e-03,\n",
            "         1.9918e-02, -3.1352e-02,  1.3569e-02,  2.6207e-02,  3.0715e-02,\n",
            "        -6.5452e-03, -2.1769e-02,  2.9113e-02,  3.5626e-02,  3.2422e-02,\n",
            "         2.1948e-02, -2.1166e-02,  1.0130e-02,  6.7064e-03,  5.2411e-03,\n",
            "        -1.5237e-02,  5.9103e-03, -1.4634e-03, -2.3639e-02,  7.5514e-03,\n",
            "        -6.5332e-03, -1.5740e-02, -2.0749e-03,  3.0456e-02, -1.1750e-02,\n",
            "         1.7390e-02,  2.1460e-02, -3.4032e-02, -4.5617e-03,  1.5206e-02,\n",
            "        -2.4248e-02,  2.9072e-02,  1.7786e-02, -6.6430e-04, -3.3829e-02,\n",
            "         8.5256e-03, -2.5217e-02, -2.9077e-02,  1.7546e-02,  4.8574e-05,\n",
            "         1.5191e-02, -1.7736e-02, -7.4796e-04,  3.2462e-02, -1.2579e-02,\n",
            "         2.6102e-02, -1.2093e-02, -1.1987e-02,  1.6971e-02,  2.1280e-02,\n",
            "         2.8678e-02,  1.2747e-02, -1.7676e-02, -2.9076e-02, -2.2868e-02,\n",
            "         1.0486e-02,  3.1600e-02,  3.5295e-02, -1.1939e-02, -4.2405e-03,\n",
            "        -1.1722e-02, -2.9398e-02, -3.4080e-02], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.0231,  0.0122, -0.0546,  ..., -0.0148,  0.0158,  0.0099],\n",
            "        [-0.0646,  0.0037,  0.0689,  ...,  0.0001, -0.0122,  0.0354],\n",
            "        [ 0.0512, -0.0179,  0.0502,  ..., -0.0676,  0.0730, -0.0719],\n",
            "        ...,\n",
            "        [ 0.0326,  0.0038, -0.0098,  ..., -0.0424,  0.0187,  0.0813],\n",
            "        [ 0.0588,  0.0465,  0.0368,  ...,  0.0578,  0.0117,  0.0734],\n",
            "        [-0.0793, -0.0012, -0.0146,  ...,  0.0156,  0.0875, -0.0469]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0158, -0.0259, -0.0484, -0.0736,  0.0236,  0.0761, -0.0757, -0.0850,\n",
            "         0.0839,  0.0582], requires_grad=True)\n",
            "W_0.shape: torch.Size([128, 784])\n",
            "W_1.shape: torch.Size([128])\n",
            "W_2.shape: torch.Size([10, 128])\n",
            "W_3.shape: torch.Size([10])\n",
            "Model Parameters check: (784+1)*128+(128+1)*10\n",
            "Model Parameters count: 101770\n",
            "Model Check count: 101770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMBJsV5INJPZ"
      },
      "source": [
        "In questo caso la loss che useremo richiede che la rete neurale abbia come output le logits, quindi la `softmax` e' applicata all'interno della loss.\n",
        "\n",
        "- Guardando la documentazione della `loss` utilizzata [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) si vede che la softmax viene applicata all'interno della loss.\n",
        "\n",
        "Usiamo poi come ottimizzatore Adam/SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrLgKoAFhky"
      },
      "source": [
        "#Define Loss and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUixvAjOTBt"
      },
      "source": [
        "# Train and Test the Model\n",
        "\n",
        "Definiamo ora la funzione per addestrare il modello e la funzione per valutare il modello (inference)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gypOQfKQFh9a"
      },
      "source": [
        "#define train class\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ongQ6r0mPbKp"
      },
      "source": [
        "Osserviamo ora come si comporta il modello per una immagine del test set, presumibilmente l'accuratezza sara' bassa, dal momento che il modello e' inizializzato casualmente. Osserviamo l'output del modello dopo la tarsformazione `softmax`, possiamo aspettarci che le probabilita' di ciascuna classe siano di valori simili, dal momento che in questo momento e' come pescare a caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "W-7xyK3QPafS",
        "outputId": "d9603d6c-9e53-408d-e585-805f83d74bcb"
      },
      "source": [
        "x, y = test_data[image_test_idx][0], test_data[image_test_idx][1]\n",
        "softmax = nn.Softmax(dim=1)\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    not_trained_softmax = softmax(pred)\n",
        "    print(\"softmax (no training done):\".format(not_trained_softmax))\n",
        "    predicted, actual = pred[0].argmax(0), y\n",
        "    print(f'Modello: \"{predicted}\", Target: \"{actual}\"')\n",
        "    plt.imshow(test_data[image_test_idx][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1028, 0.0919, 0.1004, 0.0933, 0.1016, 0.1170, 0.0962, 0.0861, 0.1066,\n",
            "         0.1041]])\n",
            "Modello: \"5\", Target: \"4\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSJt7OchFmIj",
        "outputId": "ffd6d6c6-8fba-4df9-ea46-46b75834bdbb"
      },
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296838  [    0/60000]\n",
            "loss: 0.417576  [ 6400/60000]\n",
            "loss: 0.277431  [12800/60000]\n",
            "loss: 0.341728  [19200/60000]\n",
            "loss: 0.222658  [25600/60000]\n",
            "loss: 0.320681  [32000/60000]\n",
            "loss: 0.150208  [38400/60000]\n",
            "loss: 0.340063  [44800/60000]\n",
            "loss: 0.297121  [51200/60000]\n",
            "loss: 0.305866  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 0.202457 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.129956  [    0/60000]\n",
            "loss: 0.201932  [ 6400/60000]\n",
            "loss: 0.108646  [12800/60000]\n",
            "loss: 0.184935  [19200/60000]\n",
            "loss: 0.147004  [25600/60000]\n",
            "loss: 0.234245  [32000/60000]\n",
            "loss: 0.062713  [38400/60000]\n",
            "loss: 0.239534  [44800/60000]\n",
            "loss: 0.196978  [51200/60000]\n",
            "loss: 0.205869  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 0.150069 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.088800  [    0/60000]\n",
            "loss: 0.146475  [ 6400/60000]\n",
            "loss: 0.077432  [12800/60000]\n",
            "loss: 0.101199  [19200/60000]\n",
            "loss: 0.104628  [25600/60000]\n",
            "loss: 0.184861  [32000/60000]\n",
            "loss: 0.038908  [38400/60000]\n",
            "loss: 0.170295  [44800/60000]\n",
            "loss: 0.145440  [51200/60000]\n",
            "loss: 0.158605  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 0.130096 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.071576  [    0/60000]\n",
            "loss: 0.114942  [ 6400/60000]\n",
            "loss: 0.059180  [12800/60000]\n",
            "loss: 0.057791  [19200/60000]\n",
            "loss: 0.069653  [25600/60000]\n",
            "loss: 0.154667  [32000/60000]\n",
            "loss: 0.026385  [38400/60000]\n",
            "loss: 0.117627  [44800/60000]\n",
            "loss: 0.104976  [51200/60000]\n",
            "loss: 0.126495  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.113741 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.052828  [    0/60000]\n",
            "loss: 0.090689  [ 6400/60000]\n",
            "loss: 0.049425  [12800/60000]\n",
            "loss: 0.035509  [19200/60000]\n",
            "loss: 0.058617  [25600/60000]\n",
            "loss: 0.125737  [32000/60000]\n",
            "loss: 0.018488  [38400/60000]\n",
            "loss: 0.072811  [44800/60000]\n",
            "loss: 0.083390  [51200/60000]\n",
            "loss: 0.100193  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.099969 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ysdHKLOOrBM"
      },
      "source": [
        "Siamo riusciti a raggiungere un'accuratezza `>95%`, utilizzando un modello tutto sommato semplice come architeuttra.\n",
        "\n",
        "Osserviamo ora come il modello si comporta sugli stessi dati che avevamo visto in precedenza una volta che e' stato inizializzato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "1mdwubkyG_vv",
        "outputId": "3b903410-b887-403c-c3c8-20f97cb180a6"
      },
      "source": [
        "x, y = test_data[image_test_idx][0], test_data[image_test_idx][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    trained_softmax = softmax(pred)\n",
        "    print(\"softmax (no training done): {}\".format(not_trained_softmax))\n",
        "    print(\"softmax (training done): {}\".format(trained_softmax))\n",
        "    predicted, actual = pred[0].argmax(0), y\n",
        "    print(f'Modello: \"{predicted}\", Target: \"{actual}\"')\n",
        "    plt.imshow(test_data[image_test_idx][0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax (no training done): tensor([[0.1028, 0.0919, 0.1004, 0.0933, 0.1016, 0.1170, 0.0962, 0.0861, 0.1066,\n",
            "         0.1041]])\n",
            "softmax (training done): tensor([[1.8291e-11, 3.4304e-09, 3.3936e-09, 3.8849e-07, 9.9993e-01, 2.4000e-07,\n",
            "         1.0028e-11, 4.1489e-05, 1.4073e-06, 2.7988e-05]])\n",
            "Modello: \"4\", Target: \"4\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQuR0ZM4RueL"
      },
      "source": [
        "# Conclusion\n",
        "Congratulazioni abbiamo terminato il tutorial di `PyTorch`. Per ulteriori dettagli e informazioni visita il sito ufficiale di [PyTorch](https://pytorch.org/)."
      ]
    }
  ]
}